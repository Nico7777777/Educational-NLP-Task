{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T16:46:02.122664Z",
     "start_time": "2025-01-06T16:46:01.501052Z"
    }
   },
   "outputs": [],
   "source": [
    "from ollama import chat, ChatResponse, Client\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T16:46:02.144895Z",
     "start_time": "2025-01-06T16:46:02.141604Z"
    }
   },
   "outputs": [],
   "source": [
    "annot = [\"SolutionExp\", \"HintExp\", \"AlgoExp\", \"TimeComExp\", \"FixingExp\", \"TestExp\", \"QuestionExp\", \"SubQuestion\", \"TestQuestion\", \"Irrelevant\"]\n",
    "\n",
    "general_prompt = str(\n",
    "    f\"Your are given a problem statement and a comment. Classify the comment into one of the following classes: \"\n",
    "        f\"SolutionExp: A detailed comment explaining how to solve a problem, more like a step-by-step guide. Often it contains the whole code solution\"\n",
    "        f\"HintExp: It can be viewed as a sparse variant of SolutionExp in which various elements from an entire solution are presented\"\n",
    "        f\"AlgoExp: A comment which states mere the algorithm used with little to no information\"\n",
    "        f\"TimeComExp: A comment which states mere the complexity little to no information\"\n",
    "        f\"FixingExp: A comment which explains or hints how to solve a specific bug in an implementation\"\n",
    "        f\"TestExp: A comment which explains the result of a test case\"\n",
    "        f\"QuestionExp: A relevant question about a specific task in the problem\"\n",
    "        f\"SubQuestion: A comment which asks insights about why a submission fails\"\n",
    "        f\"TestQuestion: A comment asking for insights about a specific testâ€™s result\"\n",
    "        f\"Irrelevant: Any other comment that does not fit the above labels\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T16:57:25.943323Z",
     "start_time": "2025-01-06T16:57:24.711807Z"
    }
   },
   "outputs": [],
   "source": [
    "pb_set = pd.read_csv('problemset.csv')\n",
    "disc = pd.read_csv('discussion.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precleaning the dataframe from readerbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T16:52:38.919841Z",
     "start_time": "2025-01-06T16:52:38.835957Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('annotated_labels.csv')\n",
    "new_df = df[['desc', 'sentiment', 'text']]\n",
    "new_df.to_csv('annotated_labels.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generating annotations process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:09:15.474397Z",
     "start_time": "2025-01-06T17:09:15.470694Z"
    }
   },
   "outputs": [],
   "source": [
    "correct, predicted = list(new_df['sentiment']), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:09:15.960699Z",
     "start_time": "2025-01-06T17:09:15.955772Z"
    }
   },
   "outputs": [],
   "source": [
    "def annotate(client: Client):\n",
    "    global correct, predicted\n",
    "    for _, row in new_df.iterrows():\n",
    "        # Comment prompting\n",
    "        comment = \"This is the comment: \" + row[\"text\"]\n",
    "        prompt = (general_prompt + comment + \"give me in ONLY ONE WORD the prediction for the class\")\n",
    "\n",
    "        try:\n",
    "            response = client.generate(model=\"llama3.1:70b\", prompt=prompt, system=\"You are an educational expert.\")\n",
    "            predicted.append(response['response'])\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:21:47.826892Z",
     "start_time": "2025-01-06T17:09:16.435864Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    token = open('token.txt').readline()\n",
    "    cl = Client(\n",
    "        host='https://chat.readerbench.com/ollama',\n",
    "        headers={\"Authorization\": f\"Bearer {token}\"}\n",
    "    )\n",
    "\n",
    "    annotate(cl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:34:50.766720Z",
     "start_time": "2025-01-06T17:34:50.568661Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true=correct, y_pred=predicted, labels=annot)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=annot)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix', fontsize=15, pad=20)\n",
    "plt.xlabel('Prediction', fontsize=11)\n",
    "plt.ylabel('Actual', fontsize=11)\n",
    "#Customizations\n",
    "plt.gca().xaxis.set_label_position('top')\n",
    "plt.gca().xaxis.tick_top()\n",
    "plt.gca().figure.subplots_adjust(bottom=0.2)\n",
    "plt.gca().figure.text(0.5, 0.05, 'Prediction', ha='center', fontsize=13)\n",
    "plt.savefig('confusion_matrix1.png', dpi=300, format='png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enhanced annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T19:23:33.314259Z",
     "start_time": "2025-01-06T19:23:33.308031Z"
    }
   },
   "outputs": [],
   "source": [
    "def annotate2():\n",
    "    global correct, predicted\n",
    "    for idx, row in new_df.iterrows():\n",
    "        # Comment prompting\n",
    "        comment = \"This is the comment: \" + row[\"text\"]\n",
    "        prompt = (general_prompt + comment + \"give me in ONLY ONE WORD the prediction for the class\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            cerinta = pb_set.loc[pb_set['Description'] == row['desc'], 'Problem Name'].iloc[0]\n",
    "\n",
    "            problem_lines = pb_set[pb_set['Description'].str.contains(row['desc'], regex=False)] # row['desc'] is actually the comment\n",
    "            line = problem_lines[problem_lines['Description'] == row['desc']]\n",
    "            print(idx, cerinta, line['Description'])\n",
    "            # if line['Parent']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        if idx == 6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T19:23:35.033805Z",
     "start_time": "2025-01-06T19:23:34.981756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 last-person-to-fit-in-the-bus\n",
      " 908    <p>Table: <code>Queue</code></p>\\n\\n<pre>+----...\n",
      "Name: Description, dtype: object\n",
      "1 remove-sub-folders-from-the-filesystem\n",
      " 925    <p>Given a list of folders <code>folder</code>...\n",
      "Name: Description, dtype: object\n",
      "2 cat-and-mouse-ii\n",
      " 1312    <p>A game is played by a cat and a mouse named...\n",
      "Name: Description, dtype: object\n",
      "3 number-of-ways-to-stay-in-the-same-place-after-some-steps\n",
      " 948    <p>You have a pointer at index <code>0</code> ...\n",
      "Name: Description, dtype: object\n",
      "4 find-the-duplicate-number\n",
      " 229    <p>Given an array of integers <code>nums</code...\n",
      "Name: Description, dtype: object\n",
      "5 coin-change-ii\n",
      " 400    <p>You are given an integer array <code>coins<...\n",
      "Name: Description, dtype: object\n",
      "6 flip-columns-for-maximum-number-of-equal-rows\n",
      " 831    <p>You are given an <code>m x n</code> binary ...\n",
      "Name: Description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "annotate2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
